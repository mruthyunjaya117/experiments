{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f0802bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers\n",
    "#!pip install datasets\n",
    "#!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6cddfd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\dag9kor\\anaconda3_latest\\lib\\site-packages (2.11.0)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\dag9kor\\anaconda3_latest\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\dag9kor\\anaconda3_latest\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\dag9kor\\anaconda3_latest\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\dag9kor\\anaconda3_latest\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\dag9kor\\anaconda3_latest\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\dag9kor\\anaconda3_latest\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: tensorflow-intel==2.11.0 in c:\\users\\dag9kor\\anaconda3_latest\\lib\\site-packages (from tensorflow) (2.11.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\dag9kor\\anaconda3_latest\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.19.6)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\dag9kor\\anaconda3_latest\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in c:\\users\\dag9kor\\anaconda3_latest\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.11.0)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\dag9kor\\anaconda3_latest\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.20.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\dag9kor\\anaconda3_latest\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (65.7.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\dag9kor\\anaconda3_latest\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.12.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\dag9kor\\anaconda3_latest\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\dag9kor\\anaconda3_latest\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\dag9kor\\anaconda3_latest\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\dag9kor\\anaconda3_latest\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (15.0.6.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\dag9kor\\anaconda3_latest\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (21.3)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\dag9kor\\anaconda3_latest\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (4.4.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\dag9kor\\anaconda3_latest\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.10.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\dag9kor\\anaconda3_latest\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.51.3)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\dag9kor\\anaconda3_latest\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\dag9kor\\anaconda3_latest\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.15.0)\n",
      "Requirement already satisfied: tensorboard<2.12,>=2.11 in c:\\users\\dag9kor\\anaconda3_latest\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.11.2)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\dag9kor\\anaconda3_latest\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\dag9kor\\anaconda3_latest\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (23.1.21)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\dag9kor\\anaconda3_latest\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: keras<2.12,>=2.11.0 in c:\\users\\dag9kor\\anaconda3_latest\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.11.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\dag9kor\\anaconda3_latest\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.11.0->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\dag9kor\\anaconda3_latest\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\dag9kor\\anaconda3_latest\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.16.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\dag9kor\\anaconda3_latest\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.28.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\dag9kor\\anaconda3_latest\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\dag9kor\\anaconda3_latest\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\dag9kor\\anaconda3_latest\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\dag9kor\\anaconda3_latest\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\dag9kor\\anaconda3_latest\\lib\\site-packages (from packaging->tensorflow-intel==2.11.0->tensorflow) (2.4.7)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\dag9kor\\anaconda3_latest\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (5.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\dag9kor\\anaconda3_latest\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\dag9kor\\anaconda3_latest\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\dag9kor\\anaconda3_latest\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\dag9kor\\anaconda3_latest\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (6.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\dag9kor\\anaconda3_latest\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dag9kor\\anaconda3_latest\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dag9kor\\anaconda3_latest\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dag9kor\\anaconda3_latest\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2020.12.5)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\dag9kor\\anaconda3_latest\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\dag9kor\\anaconda3_latest\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\dag9kor\\anaconda3_latest\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "#!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "634f09e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid.\n",
      "Your token has been saved in your configured git credential helpers (manager).\n",
      "Your token has been saved to C:\\Users\\DAG9KOR\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1a42231",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install git-lfs\n",
    "!git config --global user.email \"mruthyunjaya117@gmail.com\"\n",
    "!git config --global user.name \"dhanunjaya\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d7e0eb15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized empty Git repository in C:/Users/DAG9KOR/Downloads/.git/\n"
     ]
    }
   ],
   "source": [
    "!git init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4c032d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git remote add  origin https://huggingface.co/dhanunjaya/distilgpt2-finetuned-medical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "626a38d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.26.1\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "639d24ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset text (C:/Users/DAG9KOR/.cache/huggingface/datasets/text/default-f3a5ea111747c2d1/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd97899e484a4ff2a246539fa125b0db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "#datasets = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "#C:\\Users\\DAG9KOR\\Downloads\\wikitext-2-raw-v1\\wikitext-2-raw\\wiki.train.raw\n",
    "\n",
    "datasets1 = load_dataset(\"text\", data_files={\"train\": 'pragmatic_train.txt',\n",
    "                                                \"validation\": 'pragmatic_val.txt'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "558266cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Get Back to People.............................................................................34'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets1[\"train\"][91]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f4a834dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import ClassLabel\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "\n",
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(\n",
    "        dataset\n",
    "    ), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset) - 1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset) - 1)\n",
    "        picks.append(pick)\n",
    "\n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d30736c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tricked by the soldiers, who use the villagers' curiosity to get food from</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How's it going to sound to your boss?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Implementing DBC..........................................................................104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Keep Knowledge in Plain Text</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Code That's Easy to Test.....................................................................165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>•</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tools to meet new challenges?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>resource allocation and deallocation. So let us suggest a simple tip:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Select the date tab, and enter the date you noted for the Makefile in the first date field.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(datasets1[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "59caa51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"distilgpt2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bbf8695a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7d787c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [15167, 647, 6630, 832, 262, 3649, 43135, 290], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer(datasets1[\"train\"][10]['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9a9ac393",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a41cf91e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\DAG9KOR\\.cache\\huggingface\\datasets\\text\\default-f3a5ea111747c2d1\\0.0.0\\cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2\\cache-a1ffbdc698834b44.arrow\n",
      "Loading cached processed dataset at C:\\Users\\DAG9KOR\\.cache\\huggingface\\datasets\\text\\default-f3a5ea111747c2d1\\0.0.0\\cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2\\cache-3897399aa780e9f1.arrow\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = datasets1.map(\n",
    "    tokenize_function, batched=True, num_proc=1, remove_columns=[\"text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "48ee722f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [], 'attention_mask': []}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets['train'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0cb51010",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "68c151b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, though you could add padding instead if the model supports it\n",
    "    # In this, as in all things, we advise you to follow your heart\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "be527b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\DAG9KOR\\.cache\\huggingface\\datasets\\text\\default-f3a5ea111747c2d1\\0.0.0\\cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2\\cache-c9f01e0d694d5bb3.arrow\n",
      "Loading cached processed dataset at C:\\Users\\DAG9KOR\\.cache\\huggingface\\datasets\\text\\default-f3a5ea111747c2d1\\0.0.0\\cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2\\cache-d6688a96dc1d4e46.arrow\n"
     ]
    }
   ],
   "source": [
    "lm_datasets = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    num_proc=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "567aaff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"...................................................................23Challenges..........................................................................................23Stone Soup and Boiled Frogs................................................................23The Villagers' Side.............................................................................24Related sections include:................................................................25Challenges......................................................................................25Good-Enough Software.........................................................................25Involve Your Users in the Trade-Off................................................26Know When to Stop...........................................................................26Related sections include:................................................................27Challenges......................................................................................27Your Knowledge Portfolio.....................................................................27Your Knowledge Portfolio..................................................................28Building Your Portfolio......................................................................28Goals...................................................................................................29Opportunities for Learning...............................................................30Critical Thinking...............................................................................30Care and Cultivation of Gurus.................................................................30Challenges......................................................................................31Communicate!........................................................................................31Know What You Want to Say............................................................32Know Your Audience.........................................................................32Figure 1.1. The wisdom acrostic—understanding an audience...32\\x0cChoose Your Moment.........................................................................33Choose a Style....................................................................................33Make It Look Good............................................................................33Involve Your Audience.......................................................................34Be a Listener......................................................................................34Get Back to People.............................................................................34E-Mail Communication.............................................................................34Summary...............................................................................................35Related sections include:...................................................................35\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(lm_datasets[\"train\"][2][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "04a11e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at distilgpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForCausalLM\n",
    "\n",
    "model = TFAutoModelForCausalLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2f957141",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import create_optimizer, AdamWeightDecay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c5898822",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4bce83ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model.compile(optimizer=optimizer, jit_compile=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "268b03bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = model.prepare_tf_dataset(\n",
    "    lm_datasets[\"train\"],\n",
    "    shuffle=True,\n",
    "    batch_size=16,\n",
    ")\n",
    "\n",
    "validation_set = model.prepare_tf_dataset(\n",
    "    lm_datasets[\"validation\"],\n",
    "    shuffle=False,\n",
    "    batch_size=16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09a04b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning https://huggingface.co/dhanunjaya/distilgpt2-finetuned-pragmatic-10 into local empty directory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "19/19 [==============================] - 2635s 134s/step - loss: 4.2934 - val_loss: 4.1740\n",
      "Epoch 2/10\n",
      "14/19 [=====================>........] - ETA: 7:41 - loss: 4.1693"
     ]
    }
   ],
   "source": [
    "from transformers.keras_callbacks import PushToHubCallback\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "push_to_hub_model_id = f\"{model_name}-finetuned-wikitext2\"\n",
    "\n",
    "tensorboard_callback = TensorBoard(log_dir=\"./clm_model_save/logs\")\n",
    "\n",
    "push_to_hub_callback = PushToHubCallback(\n",
    "    output_dir=\"./clm_model_save\",\n",
    "    tokenizer=tokenizer,\n",
    "    #hub_model_id=push_to_hub_model_id\n",
    "    hub_model_id=\"dhanunjaya/distilgpt2-finetuned-pragmatic-10\",\n",
    ")\n",
    "\n",
    "callbacks = [tensorboard_callback, push_to_hub_callback]\n",
    "\n",
    "model.fit(train_set, validation_data=validation_set, epochs=10,callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3253e57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.to_json_file(\"config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bc3ea95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 11s 3s/step - loss: 4.7329\n"
     ]
    }
   ],
   "source": [
    "eval_loss = model.evaluate(validation_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b7fc1aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 113.62\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "print(f\"Perplexity: {math.exp(eval_loss):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d593949f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6646460d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bee41c5a8dd14f45ac1a95ff72d681ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.01k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0057dd513ac4f5cb7f020bb7bb55e12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"tf_model.h5\";:   0%|          | 0.00/328M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at dhanunjaya/distilgpt2-finetuned-wikitext2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33f430dea26e43be982bb12db15bf2cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/119 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4645a77fa41540708a6667ad3dcfbeb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/295 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "966a7d82a43848fd9cb2564125eaa6ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfd9da3baad14f51a52d89b9b3bcf49f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0146b4aca9784be7b2aa4f62388c63b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c03ce97c437a4175811a200adc4a268c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModelForCausalLM\n",
    "\n",
    "# You can, of course, use your own username and model name here \n",
    "# once you've pushed your model using the code above!\n",
    "checkpoint = \"dhanunjaya/distilgpt2-finetuned-wikitext2\"\n",
    "model = TFAutoModelForCausalLM.from_pretrained(checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7c26308f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = \"The classification of brain tumors\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "efee0355",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[  464 17923   286  3632 31155   318  1912   319   262  1271   286  4778\n",
      "    287   262  3632   326   389  1944   287   262  3632   764   383  1271\n",
      "    286  4778   287   262  3632   318  1912   319   262  1271   286  4778\n",
      "    287   262  3632   326   389  1944   287   262  3632   764   383  1271]], shape=(1, 48), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "tokenized = tokenizer(test_sentence, return_tensors=\"np\")\n",
    "\n",
    "outputs = model.generate(**tokenized, max_length=48)\n",
    "\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3501c082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The classification of brain tumors is based on the number of cells in the brain that are present in the brain. The number of cells in the brain is based on the number of cells in the brain that are present in the brain. The number'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87d9c78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
